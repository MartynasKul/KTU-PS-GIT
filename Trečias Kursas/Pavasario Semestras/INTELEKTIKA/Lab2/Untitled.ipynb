{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "693567cf-5bb4-4fbb-9d7c-1ff2e576cc65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "class Sigmoid():\n",
    "  def __init__(this):\n",
    "    this.layer_type= \"activation\"\n",
    "\n",
    "  def forward(this, X):\n",
    "    this.output = 1 / (1 + np.exp(-X))\n",
    "    return this.output\n",
    "\n",
    "  def backward(this, gradient):\n",
    "    this.gradient = this.output * (1 - this.output) * gradient\n",
    "    return this.gradient\n",
    "\n",
    "\n",
    "class Layer():\n",
    "  def __init__(this, input_size, layer_size, init=\"rand_zeros\"):\n",
    "    if init == \"rand_zeros\":\n",
    "      this.W = np.random.rand(input_size, layer_size)\n",
    "      this.b = np.zeros((1, layer_size))\n",
    "    elif init == \"randn_rand\":\n",
    "      this.W = np.random.randn(input_size, layer_size)\n",
    "      this.b = np.random.rand(1, layer_size)\n",
    "    this.layer_type= \"layer\"\n",
    "\n",
    "  def forward(this, X):\n",
    "    this.input = X\n",
    "    this.output = np.matmul(X, this.W) + this.b\n",
    "    return this.output\n",
    "\n",
    "  def backward(this, gradient):\n",
    "    this.dW = np.matmul(this.input.T, gradient)\n",
    "    this.db = np.sum(gradient, axis=0)\n",
    "    this.gradient = np.matmul(gradient, this.W.T)\n",
    "    return this.gradient\n",
    "\n",
    "  def optimize(this, learning_rate):\n",
    "    this.W = this.W - this.dW * learning_rate\n",
    "    this.b = this.b - this.db * learning_rate\n",
    "\n",
    "\n",
    "class MSE():\n",
    "  def __init__(this):\n",
    "    pass\n",
    "\n",
    "  def forward(this, y_pred, y_true):\n",
    "    this.error = y_pred - y_true\n",
    "    this.output = np.sum(this.error ** 2, axis=1)\n",
    "    return this.output\n",
    "\n",
    "  def backward(this):\n",
    "    return this.error\n",
    "\n",
    "\n",
    "class Model_Base():\n",
    "  def __init__(this, sequential):\n",
    "    this.sequential = sequential\n",
    "    this.history = {\"train_loss\": [], \"train_accuracy\": [], \"val_loss\": [], \"val_accuracy\": []}\n",
    "\n",
    "  def predict(this, X):\n",
    "    for layer in this.sequential:\n",
    "      X = layer.forward(X)\n",
    "    return X\n",
    "\n",
    "  def backward(this, gradient):\n",
    "    for layer in reversed(this.sequential):\n",
    "      gradient = layer.backward(gradient)\n",
    "\n",
    "  def optimize(this, learning_rate):\n",
    "    for layer in this.sequential:\n",
    "      if layer.layer_type == \"layer\":\n",
    "        layer.optimize(learning_rate)\n",
    "\n",
    "  def fit(this, X, y, epochs, learning_rate, loss_fn, batch_size, val_data=None, print_info=False):\n",
    "    # Training\n",
    "    for i in range(epochs):\n",
    "      indeces = np.random.choice(len(X), len(X), replace=False)\n",
    "      for j in range(len(X) // batch_size):\n",
    "        X_batch = X[indeces[j * batch_size : j * batch_size + 10]]\n",
    "        y_batch = y[indeces[j * batch_size : j * batch_size + 10]]\n",
    "        y_pred = this.predict(X_batch)\n",
    "        loss = loss_fn.forward(y_pred, y_batch)\n",
    "        gradient = loss_fn.backward()\n",
    "        this.backward(gradient)\n",
    "        this.optimize(learning_rate)\n",
    "      # Saving and printing info\n",
    "      y_pred_train = this.predict(X)\n",
    "      loss_train = loss_fn.forward(y_pred_train, y)\n",
    "      acc_train = accuracy(y_pred_train, y)\n",
    "      this.history[\"train_loss\"].append(np.sum(loss_train) / len(loss_train))\n",
    "      this.history[\"train_accuracy\"].append(acc_train)\n",
    "      if type(val_data) != type(None):\n",
    "        X_val, y_val = val_data\n",
    "        y_pred_val = this.predict(X_val)\n",
    "        loss_val = loss_fn.forward(y_pred_val, y_val)\n",
    "        acc_val = accuracy(y_pred_val, y_val)\n",
    "        this.history[\"val_loss\"].append(np.sum(loss_val) / len(loss_val))\n",
    "        this.history[\"val_accuracy\"].append(acc_val)\n",
    "        if print_info:\n",
    "          print(f\"Epoch: {i + 1} Train loss: {round(np.sum(loss_train) / len(loss_train), 2)} Train accuracy: {round(acc_train * 100, 2)}% Validation loss: {round(np.sum(loss_val) / len(loss_val), 2)} Validation accuracy: {round(acc_val * 100, 2)}%\")\n",
    "      elif print_info:\n",
    "        print(f\"Epoch: {i + 1} Train loss: {round(np.sum(loss_train) / len(loss_train), 2)} Train accuracy: {round(acc_train * 100, 2)}%\")\n",
    "\n",
    "\n",
    "def accuracy(y_pred, y_true):\n",
    "  return np.sum(np.argmax(y_pred, axis=1) == np.argmax(y_true, axis=1)) / len(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ca95fa-9c90-441f-86e7-0cc3367237f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a40596-cac4-44ce-a7d9-974eabf8b851",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a8ae0e-b8a1-4984-9bd8-5c36cfec41f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e776a8c9-6505-4ae6-8c1e-9492eadb4c18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
